# ADWS Troubleshooting Guide

## Purpose
This guide helps diagnose and resolve common issues with ADWS, including OpenCode connectivity, authentication, environment configuration, and workflow failures.

## Quick Checklist
1.  **OpenCode Server**: Is it running (`opencode serve --port 4096`) and authenticated (`opencode auth login`)?
2.  **Environment**: Are all required variables set in `.env` (Jira, Bitbucket/GitHub)?
3.  **Git**: Is your working tree clean?
4.  **Tests**: Can you run `uv run pytest` successfully?

## Common Issues & Solutions

### OpenCode Server (LLM Backend)

**Symptoms**:
-   "Connection refused" or timeouts.
-   "Model not found" errors.
-   Authentication failures (401/403).

**Diagnostics**:
```bash
# Check health
curl http://localhost:4096/health

# Check process
lsof -i :4096
```

**Solutions**:
1.  **Start Server**: `opencode serve --port 4096`
2.  **Authenticate**: `opencode auth login` (Requires GitHub Copilot subscription)
3.  **Check Models**: `opencode models list` to verify access to `claude-sonnet-4` and `claude-haiku-4.5`.
4.  **Restart**: Kill existing process (`pkill -f "opencode serve"`) and restart.

### Environment & Configuration

**Symptoms**:
-   `adw setup` fails.
-   "Missing required environment variables" error.

**Solutions**:
1.  **Verify `.env`**: Ensure it exists in the project root and contains `JIRA_SERVER`, `JIRA_USERNAME`, `JIRA_API_TOKEN`.
2.  **Check Syntax**: No spaces around `=` (e.g., `KEY=value`, not `KEY = value`).
3.  **Load Env**: Run `source .env` or ensure your shell loads it.

### Git & Repository

**Symptoms**:
-   Branch creation fails.
-   "Working tree not clean" errors.

**Solutions**:
1.  **Clean State**: Commit or stash changes before running `adw plan` or `adw build`.
2.  **Branch Name**: Ensure the target branch doesn't already exist locally or remotely if you intend to create a new one.

### Workflow Failures

**Symptoms**:
-   Plan generation hangs or returns empty.
-   Implementation fails to write files.

**Solutions**:
1.  **Check Logs**: Inspect `ADWS/logs/{adw_id}/{phase}/execution.log`.
2.  **Timeout**: Increase `timeout` in `ADWS/config.yaml` if operations are taking too long.
3.  **Permissions**: Ensure the process has write access to the source directory.

### Test Framework Configuration

**Symptoms**:
-   Tests fail to run during `adw test` phase.
-   "Test command not configured" error.
-   Test output parsing failures.

**Solutions**:
1.  **Initial Setup**: Run `adw config` and select "2. Re-detect test framework".
2.  **Verify Command**: Test your configured command manually: `npm test -- --json` or `pytest`.
3.  **Check JSON Output**: If using JSON mode, verify the output file is created at the specified path.
4.  **Install Missing Plugins**:
    - **Pytest**: `pip install pytest-json-report`
    - **Jest**: Ensure `jest` is in `package.json` dependencies
5.  **Fallback to Console Mode**: Edit `ADWS/config.yaml` and set `output_format: console` if JSON mode fails.
6.  **Validate Configuration**: Run `adw config` and choose validation to test your setup.

### Token Limit Errors

**Symptoms**:
-   Error: `prompt token count of XXXXX exceeds the limit of 128000`
-   Workflow fails during test resolution or review phase.
-   Large test output or many file changes.

**Diagnostics**:
```bash
# Check saved prompts to see what's consuming tokens
ls -lh ADWS/logs/{adw_id}/test_resolver/prompts/
cat ADWS/logs/{adw_id}/test_resolver/prompts/test_fix_*.txt | wc -w
```

**Solutions**:
1.  **Reduce Test Output**:
    - Configure JSON output mode for more compact format: `adw config`
    - Reduce test verbosity in your test framework configuration
    - Fix some tests manually to reduce the failure count

2.  **Simplify Issue Scope**:
    - Break large features into smaller sub-tasks
    - Reduce acceptance criteria in the original issue
    - Limit the number of files being modified

3.  **Use Console Mode Fallback**:
    ```bash
    adw config
    # Select: "2. Re-detect test framework"
    # Choose: "console" output format
    ```

4.  **Manual Intervention**:
    - Review the saved prompt to identify what's large
    - Manually fix obvious test failures
    - Re-run `adw test` after reducing failures

5.  **Check Model Configuration**:
    - Verify you're using the right model in `ADWS/config.yaml`
    - Claude Sonnet 4: 128K input limit
    - Claude Haiku 4.5: 128K input limit
    - Claude Opus 4: 200K input limit (if available)

**Understanding Token Limits**:
- **Input tokens**: Content sent TO the LLM (prompt + context)
- **Output tokens**: Content generated BY the LLM (response)
- Token ≈ 4 characters on average (varies by content type)
- 128K tokens ≈ 512K characters ≈ 90K-100K words

**Common Token Consumption**:
- Test output: 1K-50K tokens (depends on failure count)
- File diffs: 500-10K tokens per file
- Stack traces: 200-2K tokens per trace
- Implementation plans: 1K-5K tokens

## Gathering Debug Info

When reporting issues, please provide:
1.  **Version**: `adw --version`
2.  **Health Check**: Output of `adw setup`
3.  **Logs**: Relevant files from `ADWS/logs/` (sanitize secrets!)
4.  **Config**: Contents of `ADWS/config.yaml`
